{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script to train a neural network using Keras library.\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.layers import Conv1D\n",
    "# from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "# random.seed(datetime.now())\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a classifier of type 1 with index 1\n",
      "Load in zip file containing training data\n",
      "Extract time series from zip file\n"
     ]
    }
   ],
   "source": [
    "# model_type\n",
    "# 1: both left and right sides of time series are padded\n",
    "# 2: only left side of time series is padded\n",
    "\n",
    "# Get command line parameters\n",
    "# model_type = int(sys.argv[1])\n",
    "# kk = int(sys.argv[2]) # index for NN\n",
    "model_type = 1\n",
    "kk = 1\n",
    "\n",
    "print('Train a classifier of type {} with index {}'.format(model_type, kk))\n",
    "\n",
    "\n",
    "\n",
    "# Set size of training library and length of time series\n",
    "(lib_size, ts_len) = (200000, 1500) # Or (500000, 500) for the 500-classifier\n",
    "\n",
    "\n",
    "# keeps track of training metrics\n",
    "f1_name = 'training_results_{}_{}.txt'.format(kk, model_type)\n",
    "f2_name = 'training_results_{}_{}.csv'.format(kk, model_type)\n",
    "\n",
    "f_results= open(f1_name, \"w\")\n",
    "f_results2 = open(f2_name, \"w\")\n",
    "\n",
    "\n",
    "if model_type==1:\n",
    "    pad_left = 225 if ts_len==500 else 725\n",
    "    pad_right = 225 if ts_len==500 else 725\n",
    "\n",
    "if model_type==2:\n",
    "    pad_left = 450 if ts_len==500 else 1450\n",
    "    pad_right = 0\n",
    "    \n",
    "\n",
    "# get zipfile of time series \n",
    "print('Load in zip file containing training data')\n",
    "zf = zipfile.ZipFile('../training_data/output_full/ts_{}/combined/output_resids.zip'.format(ts_len))\n",
    "text_files = zf.infolist()\n",
    "sequences = list()\n",
    "\n",
    "\n",
    "print('Extract time series from zip file')\n",
    "tsid_vals = np.arange(1,lib_size+1)\n",
    "for tsid in tsid_vals:\n",
    "    df = pandas.read_csv(zf.open('output_resids/resids'+str(tsid)+'.csv'))\n",
    "    values = df[['Residuals']].values\n",
    "    sequences.append(values)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# Get target labels for each data sample\n",
    "df_targets = pandas.read_csv('../training_data/output_full/ts_{}/combined/labels.csv'.format(ts_len),\n",
    "                          index_col='sequence_ID')\n",
    "\n",
    "# train/validation/test split denotations\n",
    "df_groups = pandas.read_csv('../training_data/output_full/ts_{}/combined/groups.csv'.format(ts_len),\n",
    "                            index_col='sequence_ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_seq = sequences\n",
    "\n",
    "# apply train/test/validation labels\n",
    "train = [final_seq[i] for i, tsid in enumerate(tsid_vals) if df_groups['dataset_ID'].loc[tsid]==1]\n",
    "validation = [final_seq[i] for i, tsid in enumerate(tsid_vals) if df_groups['dataset_ID'].loc[tsid]==2]\n",
    "test = [final_seq[i] for i, tsid in enumerate(tsid_vals) if df_groups['dataset_ID'].loc[tsid]==3]\n",
    "\n",
    "\n",
    "train_target = [df_targets['class_label'].loc[tsid] for i, tsid in enumerate(tsid_vals) if df_groups['dataset_ID'].loc[tsid]==1]\n",
    "validation_target = [df_targets['class_label'].loc[tsid] for i, tsid in enumerate(tsid_vals) if df_groups['dataset_ID'].loc[tsid]==2]\n",
    "test_target = [df_targets['class_label'].loc[tsid] for i, tsid in enumerate(tsid_vals) if df_groups['dataset_ID'].loc[tsid]==3]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dic = {\n",
    "    \"train\": train,\n",
    "    \"train_target\": train_target,\n",
    "    \"validation\": validation,\n",
    "    \"validation_target\": validation_target,\n",
    "    \"test\": test,\n",
    "    \"test_target\": test_target\n",
    "}\n",
    "\n",
    "with open(\"preproccessed_data_uncensored.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data_dic, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"preproccessed_data_uncensored.pickle\", \"rb\") as f:\n",
    "    data_dic = pickle.load(f)\n",
    "\n",
    "\n",
    "train = data_dic['train']\n",
    "train_target = data_dic['train_target']\n",
    "validation = data_dic['validation']\n",
    "validation_target = data_dic['validation_target']\n",
    "test = data_dic['test']\n",
    "test_target = data_dic['test_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "print(\"loading data\")\n",
    "with open(\"preproccessed_data_uncensored.pickle\", \"rb\") as f:\n",
    "    data_dic = pickle.load(f)\n",
    "\n",
    "train = data_dic['train']\n",
    "train_target = data_dic['train_target']\n",
    "validation = data_dic['validation']\n",
    "validation_target = data_dic['validation_target']\n",
    "test = data_dic['test']\n",
    "test_target = data_dic['test_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# print(\"processing train\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Xy_to_lookback(train, train_target, f1_results)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# f1_results.flush()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# f2_results.flush()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# f2_results.close()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mprocessing test\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m Xy_to_lookback(test, test_target, f3_results)\n\u001b[1;32m     38\u001b[0m f3_results\u001b[39m.\u001b[39mflush()\n\u001b[1;32m     39\u001b[0m f3_results\u001b[39m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mXy_to_lookback\u001b[0;34m(data, data_target, target_file, window_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m row\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(X))\n\u001b[1;32m     11\u001b[0m \u001b[39m# wr.writerow(row)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m np\u001b[39m.\u001b[39;49msavetxt(target_file, row, delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/numpy/lib/npyio.py:1570\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1567\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMismatch between array dtype (\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m) and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1568\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mformat specifier (\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1569\u001b[0m                             \u001b[39m%\u001b[39m (\u001b[39mstr\u001b[39m(X\u001b[39m.\u001b[39mdtype), \u001b[39mformat\u001b[39m)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m-> 1570\u001b[0m         fh\u001b[39m.\u001b[39;49mwrite(v)\n\u001b[1;32m   1572\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(footer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1573\u001b[0m     footer \u001b[39m=\u001b[39m footer\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m comments)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/numpy/lib/npyio.py:1481\u001b[0m, in \u001b[0;36msavetxt.<locals>.WriteWrap.write_normal\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1479\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfh\u001b[39m.\u001b[39mwrite(v\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding))\n\u001b[0;32m-> 1481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_normal\u001b[39m(\u001b[39mself\u001b[39m, v):\n\u001b[1;32m   1482\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfh\u001b[39m.\u001b[39mwrite(asunicode(v))\n\u001b[1;32m   1484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst_write\u001b[39m(\u001b[39mself\u001b[39m, v):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def Xy_to_lookback(data, data_target, target_file, window_size=600):\n",
    "    # X = []\n",
    "    # y = []\n",
    "    # wr = csv.writer(target_file, lineterminator = '\\n')\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])-window_size-1):\n",
    "            X = data[i][j:(j+window_size)]\n",
    "            y = data_target[i]\n",
    "            row = [[y]]\n",
    "            row.extend(list(X))\n",
    "            # wr.writerow(row)\n",
    "            np.savetxt(target_file, row, delimiter=\",\")\n",
    "\n",
    "f1_name = 'xy_train.csv'\n",
    "f2_name = 'xy_validation.csv'\n",
    "f3_name = 'xy_test.csv'\n",
    "\n",
    "f1_results= open(f1_name, \"w\")\n",
    "f2_results= open(f2_name, \"w\")\n",
    "f3_results= open(f3_name, \"w\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"processing train\")\n",
    "# Xy_to_lookback(train, train_target, f1_results)\n",
    "# f1_results.flush()\n",
    "# f1_results.close()\n",
    "# print(\"processing validation\")\n",
    "# Xy_to_lookback(validation, validation_target, f2_results)\n",
    "# f2_results.flush()\n",
    "# f2_results.close()\n",
    "print(\"processing test\")\n",
    "Xy_to_lookback(test, test_target, f3_results)\n",
    "\n",
    "\n",
    "f3_results.flush()\n",
    "f3_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string '0,[0.02755083],[0.02436612],[0.0170411],[0.00749803],[0.00839069],[0.01426757],[-0.00560297],[-0.01 to float64 at row 0, column 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m\"\u001b[39;49m\u001b[39mxy_test.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/numpy/lib/npyio.py:1313\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1311\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1313\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m   1314\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m   1315\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1316\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[1;32m   1318\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/numpy/lib/npyio.py:979\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    976\u001b[0m     data \u001b[39m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m    978\u001b[0m \u001b[39mif\u001b[39;00m read_dtype_via_object_chunks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 979\u001b[0m     arr \u001b[39m=\u001b[39m _load_from_filelike(\n\u001b[1;32m    980\u001b[0m         data, delimiter\u001b[39m=\u001b[39;49mdelimiter, comment\u001b[39m=\u001b[39;49mcomment, quote\u001b[39m=\u001b[39;49mquote,\n\u001b[1;32m    981\u001b[0m         imaginary_unit\u001b[39m=\u001b[39;49mimaginary_unit,\n\u001b[1;32m    982\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols, skiplines\u001b[39m=\u001b[39;49mskiplines, max_rows\u001b[39m=\u001b[39;49mmax_rows,\n\u001b[1;32m    983\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    984\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding, filelike\u001b[39m=\u001b[39;49mfilelike,\n\u001b[1;32m    985\u001b[0m         byte_converters\u001b[39m=\u001b[39;49mbyte_converters)\n\u001b[1;32m    987\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     \u001b[39m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[39m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[39m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[39m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[39mif\u001b[39;00m filelike:\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string '0,[0.02755083],[0.02436612],[0.0170411],[0.00749803],[0.00839069],[0.01426757],[-0.00560297],[-0.01 to float64 at row 0, column 1."
     ]
    }
   ],
   "source": [
    "np.loadtxt(\"xy_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in test:\n",
    "    if len(x) != 1500:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xy_to_lookback(data, data_target, window_size=600):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])-window_size):\n",
    "            X.append([data[j:(j+window_size)]])\n",
    "            y.append(data_target[i])\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f1 = open(\"test.csv\", \"w\")\n",
    "wr = csv.writer(f1, lineterminator = '\\n')\n",
    "x = [1,2,3]\n",
    "data = [222]\n",
    "data.extend(x)\n",
    "wr.writerow(data)\n",
    "wr.writerow(data)\n",
    "f1.flush()\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3\n",
       "0  1  3  4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "  File \"/tmp/ipykernel_9711/2809543262.py\", line 1, in <module>\n",
      "  File \"/tmp/ipykernel_9711/2864672604.py\", line 6, in Xy_to_lookback\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1081, in get_records\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/pygments/styles/__init__.py\", line 89, in get_style_by_name\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1430, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1402, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1539, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1582, in _fill_cache\n",
      "OSError: [Errno 12] Cannot allocate memory: '/home/rezmiry/venv/lib/python3.10/site-packages/pygments/styles'\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = Xy_to_lookback(train, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./preproccessed_data.pickle\", \"rb\") as f:\n",
    "    data_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =data_dic['train']\n",
    "validation =data_dic['validation']\n",
    "test =data_dic['test']\n",
    "\n",
    "train_target =data_dic['train_target']\n",
    "validation_target =data_dic['validation_target']\n",
    "test_target =data_dic['test_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 600\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i])-window_size):\n",
    "        X.append([train[j:(j+window_size)]])\n",
    "        y.append(train_target[i])\n",
    "    break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
