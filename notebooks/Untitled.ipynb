{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32076fd6-cd59-433c-acdf-9ed11d90c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rezmiry/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, Reshape, multiply\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling3D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import optuna\n",
    "\n",
    "from datetime import datetime\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eceaed48-bff0-4de7-ba9d-0053c555cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "import pickle\n",
    "with open(\"../preproccessed_data.pickle\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "train = data['train']\n",
    "train_target = data['train_target']\n",
    "validation = data['validation']\n",
    "validation_target = data['validation_target']\n",
    "test = data['test']\n",
    "test_target = data['test_target']\n",
    "\n",
    "# search with a subset of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, _, train_target, _ = train_test_split(train, train_target, test_size=0.8, stratify=train_target, random_state=0)\n",
    "validation, _, validation_target, _ = train_test_split(validation, validation_target, test_size=0.8, stratify=validation_target, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e273d-d142-433c-ad5e-27e1535a682f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e36d7e1-8170-42b6-bfa8-2bf4c029bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(train_target))\n",
    "\n",
    "def squeeze_excite_block(filters,input):                      \n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se) \n",
    "    se = Dense(filters//16, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = multiply([input, se])\n",
    "    return se\n",
    "\n",
    "def build_model(trial):\n",
    "    \n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "    n_filters = trial.suggest_int(f\"n_filters\", 16, 128, step=8)\n",
    "    kernel_size = trial.suggest_int(f\"kernel_size\", 4, 10, step=1)\n",
    "    pool_size = trial.suggest_int(f\"pool_size\", 2, 4, step=1)\n",
    "    kernel_regulizer_l2 = trial.suggest_float('kernel_regulizer_l2', 1e-7, 1e-4, log=True)\n",
    "    dropout = trial.suggest_float(f'dropout', 0, 0.8, step=0.05)\n",
    "    dense_dropout = trial.suggest_float(f'dropout_dense', 0, 0.8, step=0.05)\n",
    "    \n",
    "\n",
    "\n",
    "    input_x = Input(shape=(1500,1,))\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            x = Conv1D(filters=n_filters*(2**i),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(input_x)\n",
    "        else:\n",
    "            x = Conv1D(filters=n_filters*(2**i),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = squeeze_excite_block(n_filters*(2**i),x)\n",
    "        x = Activation('relu')(x)\n",
    "        if (i+1) % 2 == 0: # every two layers one pooling\n",
    "            x = AveragePooling1D(pool_size=pool_size)(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dense_dropout)(x)\n",
    "    x = Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dense_dropout)(x)\n",
    "    x = Dense(20, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    Y_HAT = Dense(4, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_x, outputs=Y_HAT)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def batch_generator(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while True:  # Loop forever, the generator never ends\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i+1) * batch_size, num_samples)\n",
    "            yield X[start:end], y[start:end]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = build_model(trial)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"model parameters: {}K\".format(model.count_params()//1000))\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512, 1024])\n",
    "    generator = batch_generator(train, train_target, batch_size=batch_size)\n",
    "    \n",
    "    for step in range(100):\n",
    "        num_batches = len(train) // batch_size\n",
    "        for batch_id in range(num_batches):\n",
    "            X_batch, y_batch = next(generator)\n",
    "            # Now you can use X_batch and y_batch to train your model\n",
    "            model.train_on_batch(X_batch, y_batch)\n",
    "            \n",
    "        \n",
    "            # Progress bar\n",
    "            print('\\r',f\"epoch {step}/100\", 'Training progress: ', '[{0}{1}]'.format('#' * ((batch_id+1) * 50 // num_batches), '.' * (50 - ((batch_id+1) * 50 // num_batches))), f' {((batch_id+1) * 100 // num_batches)}%', end='')\n",
    "        \n",
    "    \n",
    "        # Calculate the intermediate value by using evaluate instead of score\n",
    "        train_loss, train_intermediate_value = model.evaluate(train, train_target, verbose=0)\n",
    "        val_loss, val_intermediate_value = model.evaluate(validation, validation_target, verbose=0)\n",
    "        \n",
    "        \n",
    "        trial.report(val_intermediate_value, step)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            print(\"pruned\")\n",
    "            raise optuna.TrialPruned()\n",
    "            \n",
    "        \n",
    "        print(f\" | train_acc={train_intermediate_value:.2f}, val_acc={val_intermediate_value:.2f}\")\n",
    "\n",
    "    # Calculate the final value using evaluate instead of score\n",
    "    loss, final_value = model.evaluate(validation, validation_target, verbose=0)\n",
    "    return final_value  # return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ff6122-eb01-44c4-8b6b-bde2d12d202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-14 13:00:55,607] A new study created in memory with name: no-name-62e8c331-dc33-44a0-9cb4-b7d284646e5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 57K\n",
      " epoch 0/100 Training progress:  [##################################################]  100% | train_acc=0.35, val_acc=0.35\n",
      " epoch 1/100 Training progress:  [##################################################]  100% | train_acc=0.48, val_acc=0.44\n",
      " epoch 2/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.46\n",
      " epoch 3/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.45\n",
      " epoch 4/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.49\n",
      " epoch 5/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.51\n",
      " epoch 6/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.49\n",
      " epoch 7/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.50\n",
      " epoch 8/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.51\n",
      " epoch 9/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.50\n",
      " epoch 10/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 11/100 Training progress:  [##################################################]  100% | train_acc=0.51, val_acc=0.48\n",
      " epoch 12/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.51\n",
      " epoch 13/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.51\n",
      " epoch 14/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.51\n",
      " epoch 15/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.53\n",
      " epoch 16/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.51\n",
      " epoch 17/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 18/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.54\n",
      " epoch 19/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.53\n",
      " epoch 20/100 Training progress:  [##################################################]  100% | train_acc=0.51, val_acc=0.50\n",
      " epoch 21/100 Training progress:  [##################################################]  100% | train_acc=0.49, val_acc=0.48\n",
      " epoch 22/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.51\n",
      " epoch 23/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.49\n",
      " epoch 24/100 Training progress:  [##################################################]  100% | train_acc=0.49, val_acc=0.47\n",
      " epoch 25/100 Training progress:  [##################################################]  100% | train_acc=0.52, val_acc=0.50\n",
      " epoch 26/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.51\n",
      " epoch 27/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.53\n",
      " epoch 28/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 29/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 30/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 31/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.54\n",
      " epoch 32/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.56\n",
      " epoch 33/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 34/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.51\n",
      " epoch 35/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.52\n",
      " epoch 36/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.54\n",
      " epoch 37/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.51\n",
      " epoch 38/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.54\n",
      " epoch 39/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 40/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.52\n",
      " epoch 41/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.54\n",
      " epoch 42/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.54\n",
      " epoch 43/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.54\n",
      " epoch 44/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.56\n",
      " epoch 45/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 46/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.56\n",
      " epoch 47/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.53\n",
      " epoch 48/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 49/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 50/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.55\n",
      " epoch 51/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.55\n",
      " epoch 52/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.56\n",
      " epoch 53/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.56\n",
      " epoch 54/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.54\n",
      " epoch 55/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.51\n",
      " epoch 56/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 57/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.57\n",
      " epoch 58/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.54\n",
      " epoch 59/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.54\n",
      " epoch 60/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.56\n",
      " epoch 61/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.53\n",
      " epoch 62/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.56\n",
      " epoch 63/100 Training progress:  [##################################################]  100% | train_acc=0.49, val_acc=0.49\n",
      " epoch 64/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.54\n",
      " epoch 65/100 Training progress:  [##################################################]  100% | train_acc=0.49, val_acc=0.49\n",
      " epoch 66/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 67/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.57\n",
      " epoch 68/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.53\n",
      " epoch 69/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 70/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 71/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 72/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 73/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.52\n",
      " epoch 74/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 75/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.53\n",
      " epoch 76/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.55\n",
      " epoch 77/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.54\n",
      " epoch 78/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.52\n",
      " epoch 79/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.53\n",
      " epoch 80/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.54\n",
      " epoch 81/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.50\n",
      " epoch 82/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 83/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 84/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 85/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.56\n",
      " epoch 86/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.56\n",
      " epoch 87/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.54\n",
      " epoch 88/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.56\n",
      " epoch 89/100 Training progress:  [##################################################]  100% | train_acc=0.61, val_acc=0.57\n",
      " epoch 90/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.54\n",
      " epoch 91/100 Training progress:  [##################################################]  100% | train_acc=0.58, val_acc=0.55\n",
      " epoch 92/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.53\n",
      " epoch 93/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.56\n",
      " epoch 94/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n",
      " epoch 95/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.50\n",
      " epoch 96/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.53\n",
      " epoch 97/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.49\n",
      " epoch 98/100 Training progress:  [##################################################]  100% | train_acc=0.60, val_acc=0.56\n",
      " epoch 99/100 Training progress:  [##################################################]  100% | train_acc=0.59, val_acc=0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-14 13:21:07,444] Trial 0 finished with value: 0.5456249713897705 and parameters: {'num_layers': 2, 'n_filters': 48, 'kernel_size': 7, 'pool_size': 2, 'kernel_regulizer_l2': 3.8593047842255e-05, 'dropout': 0.7000000000000001, 'dropout_dense': 0.55, 'learning_rate': 0.0039190176239533685, 'batch_size': 256}. Best is trial 0 with value: 0.5456249713897705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 308K\n",
      " epoch 0/100 Training progress:  [##################################################]  100% | train_acc=0.36, val_acc=0.31\n",
      " epoch 1/100 Training progress:  [##################################################]  100% | train_acc=0.29, val_acc=0.27\n",
      " epoch 2/100 Training progress:  [##################################################]  100% | train_acc=0.33, val_acc=0.30\n",
      " epoch 3/100 Training progress:  [##################################################]  100% | train_acc=0.36, val_acc=0.33\n",
      " epoch 4/100 Training progress:  [##################################################]  100% | train_acc=0.38, val_acc=0.33\n",
      " epoch 5/100 Training progress:  [##################################################]  100% | train_acc=0.40, val_acc=0.36\n",
      " epoch 6/100 Training progress:  [##################################################]  100% | train_acc=0.41, val_acc=0.37\n",
      " epoch 7/100 Training progress:  [##################################################]  100% | train_acc=0.44, val_acc=0.40\n",
      " epoch 8/100 Training progress:  [##################################################]  100% | train_acc=0.47, val_acc=0.42\n",
      " epoch 9/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.45\n",
      " epoch 10/100 Training progress:  [##################################################]  100% | train_acc=0.50, val_acc=0.45\n",
      " epoch 11/100 Training progress:  [##################################################]  100% | train_acc=0.52, val_acc=0.48\n",
      " epoch 12/100 Training progress:  [##################################################]  100% | train_acc=0.52, val_acc=0.48\n",
      " epoch 13/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.50\n",
      " epoch 14/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.50\n",
      " epoch 15/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.51\n",
      " epoch 16/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.51\n",
      " epoch 17/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.50\n",
      " epoch 18/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.51\n",
      " epoch 19/100 Training progress:  [##################################################]  100% | train_acc=0.54, val_acc=0.50\n",
      " epoch 20/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.52\n",
      " epoch 21/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.51\n",
      " epoch 22/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.51\n",
      " epoch 23/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.50\n",
      " epoch 24/100 Training progress:  [##################################################]  100% | train_acc=0.53, val_acc=0.51\n",
      " epoch 25/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 26/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 27/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 28/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.53\n",
      " epoch 29/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.52\n",
      " epoch 30/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 31/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.51\n",
      " epoch 32/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.51\n",
      " epoch 33/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.51\n",
      " epoch 34/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.50\n",
      " epoch 35/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.50\n",
      " epoch 36/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.50\n",
      " epoch 37/100 Training progress:  [##################################################]  100% | train_acc=0.55, val_acc=0.50\n",
      " epoch 38/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.53\n",
      " epoch 39/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.53\n",
      " epoch 40/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.52\n",
      " epoch 41/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.53\n",
      " epoch 42/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.52\n",
      " epoch 43/100 Training progress:  [##################################################]  100% | train_acc=0.56, val_acc=0.52\n",
      " epoch 44/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.52\n",
      " epoch 45/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.53\n",
      " epoch 46/100 Training progress:  [##################################################]  100% | train_acc=0.57, val_acc=0.53\n",
      " epoch 47/100 Training progress:  [#############################################.....]  91%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-14 13:37:58,647] Trial 1 failed with parameters: {'num_layers': 2, 'n_filters': 120, 'kernel_size': 9, 'pool_size': 3, 'kernel_regulizer_l2': 5.987997722234362e-06, 'dropout': 0.2, 'dropout_dense': 0.6000000000000001, 'learning_rate': 0.0003759489996569065, 'batch_size': 1024} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_87878/1367354676.py\", line 96, in objective\n",
      "    model.train_on_batch(X_batch, y_batch)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 2512, in train_on_batch\n",
      "    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/utils/tf_utils.py\", line 680, in sync_to_numpy_or_python_type\n",
      "    return tf.nest.map_structure(_to_single_numpy_or_python_type, tensors)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py\", line 917, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py\", line 917, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/utils/tf_utils.py\", line 673, in _to_single_numpy_or_python_type\n",
      "    t = t.numpy()\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 1160, in numpy\n",
      "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 1126, in _numpy\n",
      "    return self._numpy_internal()\n",
      "KeyboardInterrupt\n",
      "[W 2023-07-14 13:37:58,731] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(), pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mHyperbandPruner())\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 96\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     94\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(generator)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Now you can use X_batch and y_batch to train your model\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Progress bar\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining progress: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m ((batch_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_batches), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m-\u001b[39m ((batch_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_batches))), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((batch_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mnum_batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/engine/training.py:2512\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m   2510\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m-> 2512\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[1;32m   2514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/utils/tf_utils.py:680\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/utils/tf_utils.py:673\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 673\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\" Value: \", best_trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33c443-a570-4e64-bd4c-73041371c0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69096dc-0a97-43a1-82fb-6d512fc754e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44ee2e-ca0d-4815-acb8-6fd23699665d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e61bfd-f4df-4011-96c9-914d7b80fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f718c85-fbba-4dd3-a45e-7ff494daf01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb640765-cc8a-4452-aa86-d73964bce42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f03eb-a682-4018-9e7a-628f60ed5728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c2428-687b-4b54-8673-755e166edf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed7028d7-752f-4dac-bef3-829595519f56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train best classifier conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d4a30e-b7be-494a-b9b5-162a27fae94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_x = Input(shape=(1500,1,))\n",
    "    num_layers = 7\n",
    "    n_filters = 56\n",
    "    kernel_size = 10\n",
    "    pool_size = 8\n",
    "    kernel_regulizer_l2 = 4.4087e-06\n",
    "    dropout = 0.7\n",
    "    dense_dropout = 0.35\n",
    "    learning_rate= 0.0057992\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            x = Conv1D(filters=n_filters*(2**i),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(input_x)\n",
    "        else:\n",
    "            x = Conv1D(filters=n_filters*(2**i),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        if (i+1) % 2 == 0: # every two layers one pooling\n",
    "            x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    \n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dense_dropout)(x)\n",
    "    x = Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dense_dropout)(x)\n",
    "    x = Dense(20, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    Y_HAT = Dense(4, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_x, outputs=Y_HAT)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    # trainable_count = int(\n",
    "    #     np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "    # print('Trainable params: {:,}'.format(trainable_count))\n",
    "    print(\"model parameters: {} M\".format(model.count_params()//1000000))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce43e206-d695-4125-91a6-1651d4085f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 86 M\n"
     ]
    }
   ],
   "source": [
    "best_model = build_model(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4522b7-2c05-4653-bf42-2f796ae1d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "297/297 [==============================] - 131s 345ms/step - loss: 1.3730 - accuracy: 0.4893 - val_loss: 1.5884 - val_accuracy: 0.3531\n",
      "Epoch 2/200\n",
      "297/297 [==============================] - 89s 301ms/step - loss: 1.2459 - accuracy: 0.5532 - val_loss: 1.8133 - val_accuracy: 0.3835\n",
      "Epoch 3/200\n",
      "297/297 [==============================] - 90s 304ms/step - loss: 1.1586 - accuracy: 0.6113 - val_loss: 1.7103 - val_accuracy: 0.4354\n",
      "Epoch 4/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1402 - accuracy: 0.6274 - val_loss: 1.5076 - val_accuracy: 0.5071\n",
      "Epoch 5/200\n",
      "297/297 [==============================] - 89s 301ms/step - loss: 1.1357 - accuracy: 0.6347 - val_loss: 1.4909 - val_accuracy: 0.4959\n",
      "Epoch 6/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1459 - accuracy: 0.6367 - val_loss: 1.3606 - val_accuracy: 0.5074\n",
      "Epoch 7/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.1542 - accuracy: 0.6375 - val_loss: 1.2101 - val_accuracy: 0.6016\n",
      "Epoch 8/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1622 - accuracy: 0.6387 - val_loss: 1.5475 - val_accuracy: 0.5099\n",
      "Epoch 9/200\n",
      "297/297 [==============================] - 89s 301ms/step - loss: 1.1750 - accuracy: 0.6409 - val_loss: 1.5199 - val_accuracy: 0.5054\n",
      "Epoch 10/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.1854 - accuracy: 0.6396 - val_loss: 1.2548 - val_accuracy: 0.6018\n",
      "Epoch 11/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1956 - accuracy: 0.6405 - val_loss: 1.3309 - val_accuracy: 0.5756\n",
      "Epoch 12/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1964 - accuracy: 0.6406 - val_loss: 1.3018 - val_accuracy: 0.5824\n",
      "Epoch 13/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.2001 - accuracy: 0.6407 - val_loss: 1.2480 - val_accuracy: 0.6204\n",
      "Epoch 14/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.2101 - accuracy: 0.6409 - val_loss: 1.2948 - val_accuracy: 0.5957\n",
      "Epoch 15/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2111 - accuracy: 0.6415 - val_loss: 1.2977 - val_accuracy: 0.6020\n",
      "Epoch 16/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2201 - accuracy: 0.6414 - val_loss: 1.2912 - val_accuracy: 0.5910\n",
      "Epoch 17/200\n",
      "297/297 [==============================] - 90s 304ms/step - loss: 1.2173 - accuracy: 0.6402 - val_loss: 1.3030 - val_accuracy: 0.5727\n",
      "Epoch 18/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2196 - accuracy: 0.6410 - val_loss: 1.3272 - val_accuracy: 0.5839\n",
      "Epoch 19/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2209 - accuracy: 0.6406 - val_loss: 1.2763 - val_accuracy: 0.5931\n",
      "Epoch 20/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2167 - accuracy: 0.6417 - val_loss: 1.3967 - val_accuracy: 0.5334\n",
      "Epoch 21/200\n",
      "297/297 [==============================] - 90s 304ms/step - loss: 1.2134 - accuracy: 0.6409 - val_loss: 1.3071 - val_accuracy: 0.5847\n",
      "Epoch 22/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2080 - accuracy: 0.6419 - val_loss: 1.3191 - val_accuracy: 0.5719\n",
      "Epoch 23/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.2074 - accuracy: 0.6426 - val_loss: 1.3647 - val_accuracy: 0.5433\n",
      "Epoch 24/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.2036 - accuracy: 0.6402 - val_loss: 1.3538 - val_accuracy: 0.5614\n",
      "Epoch 25/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.2016 - accuracy: 0.6425 - val_loss: 1.3041 - val_accuracy: 0.5773\n",
      "Epoch 26/200\n",
      "297/297 [==============================] - 89s 301ms/step - loss: 1.1999 - accuracy: 0.6420 - val_loss: 1.3378 - val_accuracy: 0.5680\n",
      "Epoch 27/200\n",
      "297/297 [==============================] - 89s 301ms/step - loss: 1.1906 - accuracy: 0.6415 - val_loss: 1.2819 - val_accuracy: 0.5974\n",
      "Epoch 28/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1893 - accuracy: 0.6420 - val_loss: 1.3091 - val_accuracy: 0.5714\n",
      "Epoch 29/200\n",
      "297/297 [==============================] - 90s 302ms/step - loss: 1.1887 - accuracy: 0.6414 - val_loss: 1.3025 - val_accuracy: 0.5690\n",
      "Epoch 30/200\n",
      "297/297 [==============================] - 90s 305ms/step - loss: 1.1859 - accuracy: 0.6413 - val_loss: 1.3240 - val_accuracy: 0.5483\n",
      "Epoch 31/200\n",
      "297/297 [==============================] - 90s 302ms/step - loss: 1.1801 - accuracy: 0.6430 - val_loss: 1.2947 - val_accuracy: 0.5665\n",
      "Epoch 32/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.1767 - accuracy: 0.6439 - val_loss: 1.2748 - val_accuracy: 0.5943\n",
      "Epoch 33/200\n",
      "297/297 [==============================] - 91s 306ms/step - loss: 1.1743 - accuracy: 0.6428 - val_loss: 1.3088 - val_accuracy: 0.5591\n",
      "Epoch 34/200\n",
      "297/297 [==============================] - 89s 300ms/step - loss: 1.1717 - accuracy: 0.6428 - val_loss: 1.3472 - val_accuracy: 0.5366\n",
      "Epoch 35/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1715 - accuracy: 0.6414 - val_loss: 1.2563 - val_accuracy: 0.5956\n",
      "Epoch 36/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1663 - accuracy: 0.6425 - val_loss: 1.3385 - val_accuracy: 0.5228\n",
      "Epoch 37/200\n",
      "297/297 [==============================] - 90s 302ms/step - loss: 1.1646 - accuracy: 0.6426 - val_loss: 1.2985 - val_accuracy: 0.5701\n",
      "Epoch 38/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1622 - accuracy: 0.6425 - val_loss: 1.3537 - val_accuracy: 0.5200\n",
      "Epoch 39/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1594 - accuracy: 0.6423 - val_loss: 1.3950 - val_accuracy: 0.5019\n",
      "Epoch 40/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.1575 - accuracy: 0.6438 - val_loss: 1.2698 - val_accuracy: 0.5726\n",
      "Epoch 41/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1575 - accuracy: 0.6436 - val_loss: 1.3185 - val_accuracy: 0.5397\n",
      "Epoch 42/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1550 - accuracy: 0.6434 - val_loss: 1.2956 - val_accuracy: 0.5608\n",
      "Epoch 43/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1512 - accuracy: 0.6433 - val_loss: 1.2965 - val_accuracy: 0.5564\n",
      "Epoch 44/200\n",
      "297/297 [==============================] - 91s 305ms/step - loss: 1.1489 - accuracy: 0.6426 - val_loss: 1.3183 - val_accuracy: 0.5311\n",
      "Epoch 45/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1481 - accuracy: 0.6432 - val_loss: 1.2813 - val_accuracy: 0.5616\n",
      "Epoch 46/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1488 - accuracy: 0.6429 - val_loss: 1.3131 - val_accuracy: 0.5395\n",
      "Epoch 47/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.1464 - accuracy: 0.6433 - val_loss: 1.2802 - val_accuracy: 0.5675\n",
      "Epoch 48/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1436 - accuracy: 0.6458 - val_loss: 1.2937 - val_accuracy: 0.5518\n",
      "Epoch 49/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1443 - accuracy: 0.6433 - val_loss: 1.2697 - val_accuracy: 0.5744\n",
      "Epoch 50/200\n",
      "297/297 [==============================] - 90s 303ms/step - loss: 1.1411 - accuracy: 0.6433 - val_loss: 1.2803 - val_accuracy: 0.5688\n",
      "Epoch 51/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1388 - accuracy: 0.6441 - val_loss: 1.3087 - val_accuracy: 0.5466\n",
      "Epoch 52/200\n",
      "297/297 [==============================] - 89s 299ms/step - loss: 1.1356 - accuracy: 0.6464 - val_loss: 1.2880 - val_accuracy: 0.5648\n",
      "Epoch 53/200\n",
      "273/297 [==========================>...] - ETA: 7s - loss: 1.1380 - accuracy: 0.6442"
     ]
    }
   ],
   "source": [
    "# chk = ModelCheckpoint(model_name, monitor='val_accuracy', save_best_only=True, mode='max', verbose=2)\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100)\n",
    "best_model.fit(train, train_target, epochs=200, batch_size=640, callbacks=[es], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629707a6-d75c-44c3-9c3e-9557c5f605ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bcae9-7570-422e-8f37-0b1584354f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671798b0-8bad-4d4b-b969-e0fa2dbae370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d94fabfb-fe2d-4bc8-92f6-2cf5609180b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab9a6ad-61af-45cc-a64d-f40b9ac8a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    embed_dim = hp.Int(\"embed_dim\", 16, 128, step=16) #32  # Embedding size for each token\n",
    "    num_heads = hp.Int(\"num_heads\", 2, 8, step=2) #2  # Number of attention heads\n",
    "    ff_dim = hp.Int(\"ff_dim\", 16, 128) #32  # Hidden layer size in feed forward network inside transformer\n",
    "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "    \n",
    "    inputs = layers.Input(shape=(1500, 1))  # assume we have 128 time steps and 10 features\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(inputs)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(30, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(4, activation=\"softmax\")(x)  # assume we have 3 classes\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    # print(\"model parameters: \" , model.count_params()/1000)\n",
    "    print(model.summary())    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4862c38-0f88-4e2b-a110-e8038885d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1500, 1)]         0         \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  (None, 1500, 16)         976       \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 30)                510       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 124       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,610\n",
      "Trainable params: 1,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hp_name = \"hp_transformer_test\"\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=500,\n",
    "#     hyperband_iterations=2,\n",
    "#     directory=f\"./my_hp_results/{hp_name}\",\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "# tuner.search(train,\n",
    "#     train_target,\n",
    "#     validation_data=(validation,validation_target),\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     verbose=2,\n",
    "#     callbacks=[EarlyStopping(patience=1)])\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=200,\n",
    "    executions_per_trial=2,\n",
    "    directory=f\"./my_hp_results/{hp_name}_bayesian\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6feb9e0-3a60-40ee-89b4-5db16b15ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 34m 50s]\n",
      "val_accuracy: 0.45000000298023224\n",
      "\n",
      "Best val_accuracy So Far: 0.45000000298023224\n",
      "Total elapsed time: 02h 00m 06s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "32                |32                |embed_dim\n",
      "4                 |8                 |num_heads\n",
      "112               |39                |ff_dim\n",
      "0.00025881        |0.0010482         |learning_rate\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1500, 1)]         0         \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 1500, 32)         8688      \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                990       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 124       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,802\n",
      "Trainable params: 9,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0379s vs `on_train_batch_end` time: 0.0433s). Check your callbacks.\n",
      "594/594 - 62s - loss: 1.2964 - accuracy: 0.3775 - val_loss: 1.2930 - val_accuracy: 0.3731 - 62s/epoch - 105ms/step\n",
      "Epoch 2/10\n",
      "594/594 - 54s - loss: 1.2729 - accuracy: 0.3866 - val_loss: 1.2910 - val_accuracy: 0.3787 - 54s/epoch - 91ms/step\n",
      "Epoch 3/10\n",
      "594/594 - 54s - loss: 1.2667 - accuracy: 0.3922 - val_loss: 1.2920 - val_accuracy: 0.3719 - 54s/epoch - 91ms/step\n",
      "Epoch 4/10\n",
      "594/594 - 54s - loss: 1.2616 - accuracy: 0.3957 - val_loss: 1.2844 - val_accuracy: 0.3794 - 54s/epoch - 91ms/step\n",
      "Epoch 5/10\n",
      "594/594 - 54s - loss: 1.2543 - accuracy: 0.4052 - val_loss: 1.2772 - val_accuracy: 0.3938 - 54s/epoch - 91ms/step\n",
      "Epoch 6/10\n",
      "594/594 - 55s - loss: 1.2467 - accuracy: 0.4108 - val_loss: 1.2726 - val_accuracy: 0.3825 - 55s/epoch - 93ms/step\n",
      "Epoch 7/10\n",
      "594/594 - 54s - loss: 1.2363 - accuracy: 0.4164 - val_loss: 1.2794 - val_accuracy: 0.3869 - 54s/epoch - 91ms/step\n",
      "Epoch 8/10\n",
      "594/594 - 54s - loss: 1.2294 - accuracy: 0.4266 - val_loss: 1.2615 - val_accuracy: 0.4038 - 54s/epoch - 92ms/step\n",
      "Epoch 9/10\n",
      "594/594 - 54s - loss: 1.2217 - accuracy: 0.4318 - val_loss: 1.2661 - val_accuracy: 0.3944 - 54s/epoch - 91ms/step\n",
      "Epoch 10/10\n",
      "594/594 - 54s - loss: 1.2135 - accuracy: 0.4381 - val_loss: 1.2841 - val_accuracy: 0.4019 - 54s/epoch - 91ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1500, 1)]         0         \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 1500, 32)         8688      \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                990       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 124       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,802\n",
      "Trainable params: 9,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0379s vs `on_train_batch_end` time: 0.0433s). Check your callbacks.\n",
      "594/594 - 62s - loss: 1.2971 - accuracy: 0.3744 - val_loss: 1.2932 - val_accuracy: 0.3694 - 62s/epoch - 104ms/step\n",
      "Epoch 2/10\n",
      "594/594 - 54s - loss: 1.2725 - accuracy: 0.3868 - val_loss: 1.2968 - val_accuracy: 0.3719 - 54s/epoch - 91ms/step\n",
      "Epoch 3/10\n",
      "594/594 - 54s - loss: 1.2653 - accuracy: 0.3939 - val_loss: 1.2910 - val_accuracy: 0.3787 - 54s/epoch - 91ms/step\n",
      "Epoch 4/10\n",
      "594/594 - 54s - loss: 1.2621 - accuracy: 0.3935 - val_loss: 1.2872 - val_accuracy: 0.3762 - 54s/epoch - 91ms/step\n",
      "Epoch 5/10\n",
      "594/594 - 54s - loss: 1.2559 - accuracy: 0.4026 - val_loss: 1.2835 - val_accuracy: 0.3775 - 54s/epoch - 91ms/step\n",
      "Epoch 6/10\n",
      "594/594 - 55s - loss: 1.2503 - accuracy: 0.4044 - val_loss: 1.2833 - val_accuracy: 0.3713 - 55s/epoch - 93ms/step\n",
      "Epoch 7/10\n",
      "594/594 - 54s - loss: 1.2437 - accuracy: 0.4113 - val_loss: 1.2775 - val_accuracy: 0.3925 - 54s/epoch - 91ms/step\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train,\n",
    "    train_target,\n",
    "    validation_data=(validation,validation_target),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    "    callbacks=[EarlyStopping(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f9dd2-cb63-41b8-a76c-e2208d90a679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4465a-ed45-4549-857e-d4809c4a3e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07919b9-7527-46ae-87a7-282aadf086d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af63eb5-a394-4f68-9047-589ea1660b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f313070-02b7-4248-8979-1e8f0385ec07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fc200-df38-477f-9785-f02878082714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d128b-a942-49e1-91a6-e0805fe846e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a921bceb-64cd-46b9-8806-4095ea2e1370",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### par_lstmcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e7b5d1-3766-4873-93f5-f17e0a8ac8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    n_lstm_layers = hp.Int(\"n_lstm_layers\", 1, 3)\n",
    "    mm_cells = hp.Int(f\"mem_cells\", min_value=10, max_value=100, step=10)\n",
    "\n",
    "    n_conv_layers = hp.Int(\"n_conv_layers\", 2, 8)    \n",
    "    n_filters = hp.Int(f\"n_filters\", min_value=16, max_value=128, step=8)\n",
    "    kernel_size = hp.Int(f\"kernel_size\", min_value=6, max_value=15, step=1)\n",
    "    pool_size = hp.Int(f\"pool_size\", min_value=2, max_value=8, step=1)\n",
    "    use_global_averaging = hp.Choice('use_global_averaging', [True, False])\n",
    "\n",
    "    # recurrent_dropout = hp.Float(f'recurrent_dropout', 0, 0.8, step=0.05)\n",
    "    recurrent_dropout = 0\n",
    "    lstm_dropout = hp.Float(f'lstm_dropout', 0, 0.8, step=0.05, default=0.5)\n",
    "    kernel_regulizer_l2 = hp.Float('kernel_regulizer_l2', 1e-7, 1e-1, sampling='log')\n",
    "    cnn_dropout = hp.Float(f'cnn_dropout', 0, 0.8, step=0.05, default=0.5)\n",
    "\n",
    "    learning_rate = hp.Float('learning_rate', 1e-7, 1e-2, sampling='log')\n",
    "\n",
    "\n",
    "    ip = Input(shape=(1500, 1))\n",
    "    for i in range(n_lstm_layers):\n",
    "        if i == 0:\n",
    "            x = LSTM(mm_cells,\n",
    "                    recurrent_dropout = recurrent_dropout,\n",
    "                    return_sequences= i!=(n_lstm_layers-1),\n",
    "                    kernel_initializer = 'lecun_normal')(ip)\n",
    "        else:\n",
    "            x = LSTM(mm_cells,\n",
    "                    recurrent_dropout =  recurrent_dropout,\n",
    "                    return_sequences= i!=(n_lstm_layers-1),\n",
    "                    kernel_initializer = 'lecun_normal')(x)\n",
    "        \n",
    "        if i!=(n_lstm_layers-1): # don't add dropout at the last layer\n",
    "            x = Dropout(lstm_dropout)(x)\n",
    "    \n",
    "    # y = Permute((2, 1))(ip)\n",
    "    for j in range(n_conv_layers):\n",
    "        if j == 0:\n",
    "            y = Conv1D(filters=n_filters*(2**j),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(ip)\n",
    "        else:\n",
    "            y = Conv1D(filters=n_filters*(2**j),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        if (j+1) % 2 == 0: # every two layers one pooling\n",
    "            y = AveragePooling1D(pool_size=pool_size)(y)\n",
    "        y = Dropout(cnn_dropout)(y)\n",
    "    \n",
    "    if use_global_averaging:\n",
    "        y = GlobalAveragePooling1D()(y)\n",
    "    else:\n",
    "        y = Flatten()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "    out = Dense(4, activation='softmax')(x)\n",
    "    model = Model(ip, out)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(\"model parameters: \" , model.count_params()/1000)\n",
    "    # print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc4ed96f-8f58-40b2-a126-2b918d4426a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:  4.06\n"
     ]
    }
   ],
   "source": [
    "hp_name = \"hp_parlstmcnn_test\"\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=500,\n",
    "#     hyperband_iterations=2,\n",
    "#     directory=f\"./my_hp_results/{hp_name}\",\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "# tuner.search(train,\n",
    "#     train_target,\n",
    "#     validation_data=(validation,validation_target),\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     verbose=2,\n",
    "#     callbacks=[EarlyStopping(patience=1)])\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=200,\n",
    "    executions_per_trial=2,\n",
    "    directory=f\"./my_hp_results/{hp_name}_bayesian\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f5b375-4e6b-436c-8b3a-3ef29e048198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 32m 55s]\n",
      "val_accuracy: 0.3734374940395355\n",
      "\n",
      "Best val_accuracy So Far: 0.3734374940395355\n",
      "Total elapsed time: 00h 37m 06s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |1                 |n_lstm_layers\n",
      "20                |50                |mem_cells\n",
      "8                 |2                 |n_conv_layers\n",
      "32                |120               |n_filters\n",
      "15                |14                |kernel_size\n",
      "3                 |8                 |pool_size\n",
      "1                 |1                 |use_global_averaging\n",
      "0.35              |0.05              |lstm_dropout\n",
      "0.0012172         |2.4396e-06        |kernel_regulizer_l2\n",
      "0.8               |0.7               |cnn_dropout\n",
      "2.0494e-05        |7.0486e-06        |learning_rate\n",
      "\n",
      "model parameters:  167824.708\n",
      "Epoch 1/100\n",
      "38/38 - 241s - loss: 11.2604 - accuracy: 0.2669 - val_loss: 11.1929 - val_accuracy: 0.2500 - 241s/epoch - 6s/step\n",
      "Epoch 2/100\n",
      "38/38 - 107s - loss: 10.9960 - accuracy: 0.2876 - val_loss: 11.2856 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 3/100\n",
      "38/38 - 107s - loss: 10.7690 - accuracy: 0.2944 - val_loss: 11.2815 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 4/100\n",
      "38/38 - 107s - loss: 10.5519 - accuracy: 0.2984 - val_loss: 11.1270 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 5/100\n",
      "38/38 - 107s - loss: 10.3340 - accuracy: 0.3117 - val_loss: 10.8823 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 6/100\n",
      "38/38 - 108s - loss: 10.1209 - accuracy: 0.3302 - val_loss: 10.6947 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 7/100\n",
      "38/38 - 107s - loss: 9.9028 - accuracy: 0.3538 - val_loss: 10.6831 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 8/100\n",
      "38/38 - 108s - loss: 9.6926 - accuracy: 0.3721 - val_loss: 10.9522 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 9/100\n",
      "38/38 - 106s - loss: 9.4841 - accuracy: 0.3944 - val_loss: 10.8526 - val_accuracy: 0.2500 - 106s/epoch - 3s/step\n",
      "Epoch 10/100\n",
      "38/38 - 106s - loss: 9.2835 - accuracy: 0.4143 - val_loss: 10.4865 - val_accuracy: 0.2500 - 106s/epoch - 3s/step\n",
      "Epoch 11/100\n",
      "38/38 - 107s - loss: 9.0943 - accuracy: 0.4210 - val_loss: 10.2547 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 12/100\n",
      "38/38 - 107s - loss: 8.9136 - accuracy: 0.4268 - val_loss: 9.9079 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 13/100\n",
      "38/38 - 107s - loss: 8.7343 - accuracy: 0.4342 - val_loss: 9.6019 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 14/100\n",
      "38/38 - 107s - loss: 8.5601 - accuracy: 0.4358 - val_loss: 9.1524 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 15/100\n",
      "38/38 - 107s - loss: 8.3901 - accuracy: 0.4398 - val_loss: 8.9799 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 16/100\n",
      "38/38 - 107s - loss: 8.2235 - accuracy: 0.4411 - val_loss: 8.6983 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 17/100\n",
      "38/38 - 108s - loss: 8.0641 - accuracy: 0.4404 - val_loss: 8.5412 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 18/100\n",
      "38/38 - 107s - loss: 7.9032 - accuracy: 0.4432 - val_loss: 8.5186 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 19/100\n",
      "38/38 - 107s - loss: 7.7435 - accuracy: 0.4508 - val_loss: 8.3094 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 20/100\n",
      "38/38 - 108s - loss: 7.5967 - accuracy: 0.4495 - val_loss: 8.0967 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 21/100\n",
      "38/38 - 107s - loss: 7.4423 - accuracy: 0.4554 - val_loss: 8.0680 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 22/100\n",
      "38/38 - 107s - loss: 7.2977 - accuracy: 0.4555 - val_loss: 7.9221 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 23/100\n",
      "38/38 - 108s - loss: 7.1536 - accuracy: 0.4572 - val_loss: 7.7982 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 24/100\n",
      "38/38 - 107s - loss: 7.0146 - accuracy: 0.4588 - val_loss: 7.4951 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 25/100\n",
      "38/38 - 107s - loss: 6.8778 - accuracy: 0.4603 - val_loss: 7.5485 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 26/100\n",
      "38/38 - 108s - loss: 6.7427 - accuracy: 0.4623 - val_loss: 7.3193 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 27/100\n",
      "38/38 - 107s - loss: 6.6132 - accuracy: 0.4618 - val_loss: 7.1173 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 28/100\n",
      "38/38 - 107s - loss: 6.4814 - accuracy: 0.4690 - val_loss: 6.9591 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 29/100\n",
      "38/38 - 110s - loss: 6.3583 - accuracy: 0.4694 - val_loss: 6.7238 - val_accuracy: 0.2506 - 110s/epoch - 3s/step\n",
      "Epoch 30/100\n",
      "38/38 - 107s - loss: 6.2369 - accuracy: 0.4682 - val_loss: 6.6197 - val_accuracy: 0.2506 - 107s/epoch - 3s/step\n",
      "Epoch 31/100\n",
      "38/38 - 108s - loss: 6.1140 - accuracy: 0.4723 - val_loss: 6.4895 - val_accuracy: 0.2500 - 108s/epoch - 3s/step\n",
      "Epoch 32/100\n",
      "38/38 - 107s - loss: 5.9964 - accuracy: 0.4739 - val_loss: 6.2810 - val_accuracy: 0.2506 - 107s/epoch - 3s/step\n",
      "Epoch 33/100\n",
      "38/38 - 107s - loss: 5.8849 - accuracy: 0.4727 - val_loss: 6.2660 - val_accuracy: 0.2506 - 107s/epoch - 3s/step\n",
      "Epoch 34/100\n",
      "38/38 - 108s - loss: 5.7758 - accuracy: 0.4695 - val_loss: 6.0334 - val_accuracy: 0.2506 - 108s/epoch - 3s/step\n",
      "Epoch 35/100\n",
      "38/38 - 107s - loss: 5.6620 - accuracy: 0.4746 - val_loss: 5.9998 - val_accuracy: 0.2506 - 107s/epoch - 3s/step\n",
      "Epoch 36/100\n",
      "38/38 - 107s - loss: 5.5533 - accuracy: 0.4788 - val_loss: 5.8199 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 37/100\n",
      "38/38 - 111s - loss: 5.4525 - accuracy: 0.4733 - val_loss: 5.6930 - val_accuracy: 0.2512 - 111s/epoch - 3s/step\n",
      "Epoch 38/100\n",
      "38/38 - 110s - loss: 5.3474 - accuracy: 0.4755 - val_loss: 5.6455 - val_accuracy: 0.2519 - 110s/epoch - 3s/step\n",
      "Epoch 39/100\n",
      "38/38 - 107s - loss: 5.2495 - accuracy: 0.4754 - val_loss: 5.5350 - val_accuracy: 0.2500 - 107s/epoch - 3s/step\n",
      "Epoch 40/100\n",
      "38/38 - 107s - loss: 5.1524 - accuracy: 0.4823 - val_loss: 5.5157 - val_accuracy: 0.2506 - 107s/epoch - 3s/step\n",
      "Epoch 41/100\n",
      "38/38 - 112s - loss: 5.0523 - accuracy: 0.4811 - val_loss: 5.2578 - val_accuracy: 0.2537 - 112s/epoch - 3s/step\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 235\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    238\u001b[0m     ):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    251\u001b[0m         )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    286\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 287\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py:214\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 214\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m tuner_utils\u001b[38;5;241m.\u001b[39mvalidate_trial_results(\n\u001b[1;32m    216\u001b[0m     results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperModel.fit()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(train,\n",
    "    train_target,\n",
    "    validation_data=(validation,validation_target),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    verbose=2,\n",
    "    callbacks=[EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1d264-0891-4923-9529-f8cf56a19d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15397cdc-2de1-42f0-bc68-0fab14548bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac61883-b8ec-43e0-8a6a-d28209e28f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ecce2a7-bfc1-4f6a-a18b-afdf04a2390b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Conv1dse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f846e5-e316-4182-9887-de4fa8b70360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(filters,input):                      \n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se) \n",
    "    se = Dense(filters//16, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = multiply([input, se])\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee4212e3-07e1-448e-95e3-5e03b200152d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_x = Input(shape=(1500,1,))\n",
    "    num_layers = hp.Int(\"num_layers\", 2, 8)\n",
    "    n_filters = hp.Int(f\"n_filters\", min_value=16, max_value=128, step=8)\n",
    "    kernel_size = hp.Int(f\"kernel_size\", min_value=4, max_value=15, step=1)\n",
    "    pool_size = hp.Int(f\"pool_size\", min_value=2, max_value=8, step=1)\n",
    "    kernel_regulizer_l2 = hp.Float('kernel_regulizer_l2', 1e-7, 1e-1, sampling='log')\n",
    "    dropout = hp.Float(f'dropout', 0, 0.8, step=0.05, default=0.5)\n",
    "    dense_dropout = hp.Float(f'dropout_dense', 0, 0.8, step=0.05, default=0.5)\n",
    "    learning_rate=hp.Float('learning_rate', 1e-7, 1e-2, sampling='log')\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            x = Conv1D(filters=n_filters*(2**i),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(input_x)\n",
    "        else:\n",
    "            x = Conv1D(filters=n_filters*(2**i),\n",
    "                    kernel_size=kernel_size,\n",
    "                    kernel_initializer = 'lecun_normal',\n",
    "                    kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "                    padding='same')(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = squeeze_excite_block(n_filters*(2**i),x)\n",
    "        x = Activation('relu')(x)\n",
    "        if (i+1) % 2 == 0: # every two layers one pooling\n",
    "            x = AveragePooling1D(pool_size=pool_size)(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dense_dropout)(x)\n",
    "    x = Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dense_dropout)(x)\n",
    "    x = Dense(20, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(kernel_regulizer_l2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    Y_HAT = Dense(4, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_x, outputs=Y_HAT)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(\"model parameters: {}K\".format(model.count_params()//1000))\n",
    "    # print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d77d3-6b80-4e39-93f2-5eaeb7f1f791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d3ed20-8b9d-45ef-8b61-4c85e2036a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 18K\n"
     ]
    }
   ],
   "source": [
    "hp_name = \"hp_conv1dse_test\"\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=500,\n",
    "#     hyperband_iterations=2,\n",
    "#     directory=f\"./my_hp_results/{hp_name}\",\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "# tuner.search(train,\n",
    "#     train_target,\n",
    "#     validation_data=(validation,validation_target),\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     verbose=2,\n",
    "#     callbacks=[EarlyStopping(patience=1)])\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=200,\n",
    "    executions_per_trial=2,\n",
    "    directory=f\"./my_hp_results/{hp_name}_bayesian\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f1531f9-caaf-44e6-9690-172f6d93bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "7                 |7                 |num_layers\n",
      "72                |72                |n_filters\n",
      "11                |11                |kernel_size\n",
      "6                 |6                 |pool_size\n",
      "0.021848          |0.021848          |kernel_regulizer_l2\n",
      "0.45              |0.45              |dropout\n",
      "0.55              |0.55              |dropout_dense\n",
      "1.0376e-05        |1.0376e-05        |learning_rate\n",
      "\n",
      "model parameters: 159744K\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0862s vs `on_train_batch_end` time: 0.1352s). Check your callbacks.\n",
      "297/297 - 112s - loss: 186.2729 - accuracy: 0.2784 - val_loss: 165.7838 - val_accuracy: 0.2512 - 112s/epoch - 377ms/step\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 235\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    238\u001b[0m     ):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    251\u001b[0m         )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    286\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 287\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py:214\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 214\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m tuner_utils\u001b[38;5;241m.\u001b[39mvalidate_trial_results(\n\u001b[1;32m    216\u001b[0m     results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperModel.fit()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/callbacks.py:390\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    388\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 390\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._process_logs\u001b[0;34m(self, logs, is_batch_hook)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batch_hook \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_hooks_support_tf_logs:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/utils/tf_utils.py:680\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/utils/tf_utils.py:673\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 673\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(train,\n",
    "    train_target,\n",
    "    validation_data=(validation,validation_target),\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    verbose=2,\n",
    "    callbacks=[EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291fc43-c5d2-4520-b232-c9f580ecd52d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Debug convlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fb2b72-eccf-489a-8f2b-d914de655f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1\n",
    "n_length = 1500\n",
    "n_features = 1\n",
    "n_outputs = 4\n",
    "train = train.reshape((train.shape[0], n_steps, 1, n_length, n_features))\n",
    "validation = validation.reshape((validation.shape[0], n_steps, 1, n_length, n_features))\n",
    "test = test.reshape((test.shape[0], n_steps, 1, n_length, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7830155c-10e6-4382-8ebd-ae8472042ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 2)\n",
    "    # n_filters = hp.Int(f\"n_filters\", min_value=8, max_value=32, step=8)\n",
    "    n_filters = 32\n",
    "    kernel_size = 12\n",
    "    # kernel_size = hp.Int(f\"kernel_size\", min_value=4, max_value=16, step=4)\n",
    "    kernel_regulizer_l2 = hp.Float('kernel_regulizer_l2', 1e-7, 1e-1, sampling='log')\n",
    "    recurrent_regulizer_l2 = hp.Float('recurrent_regulizer_l2', 1e-7, 1e-1, sampling='log')\n",
    "    dense_regulizer_l2 = hp.Float('dense_regulizer_l2', 1e-7, 1e-1, sampling='log')\n",
    "    dropout = hp.Float(f'dropout', 0, 0.8, step=0.05, default=0.5)\n",
    "    # recurrent_dropout = hp.Float(f'recurrent_dropout', 0, 0.8, step=0.1, default=0)\n",
    "    recurrent_dropout = 0\n",
    "    dense_dropout = hp.Float(f'dropout_dense', 0, 0.8, step=0.05, default=0.5)\n",
    "    # learning_rate=hp.Float('learning_rate', 1e-7, 1e-2, sampling='log')\n",
    "    learning_rate=0.001\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(ConvLSTM2D(\n",
    "            filters=n_filters*(2**i),\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "            recurrent_regularizer=regularizers.l2(recurrent_regulizer_l2),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            return_sequences=True,\n",
    "            input_shape=(n_steps, 1, n_length, n_features)))\n",
    "        elif i < (num_layers-1):\n",
    "            model.add(ConvLSTM2D(\n",
    "            filters=n_filters*(2**i),\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "            recurrent_regularizer=regularizers.l2(recurrent_regulizer_l2),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            return_sequences=True))\n",
    "        else:\n",
    "            model.add(ConvLSTM2D(\n",
    "            filters=n_filters*(2**i),\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=regularizers.l2(kernel_regulizer_l2),\n",
    "            recurrent_regularizer=regularizers.l2(recurrent_regulizer_l2),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            return_sequences=False))\n",
    "            \n",
    "    model.add(GlobalAveragePooling3D())\n",
    "    model.add(Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(dense_regulizer_l2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "    model.add(Dense(100, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(dense_regulizer_l2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "    model.add(Dense(20, kernel_initializer = 'lecun_normal', kernel_regularizer=regularizers.l2(dense_regulizer_l2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4, activation=\"softmax\"))\n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    # print(\"model parameters: {}K\".format(model.count_params()//1000))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d27020-1755-4a11-96ae-ffefda3ba50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 1, 1, 1500, 32)    608384    \n",
      "                                                                 \n",
      " global_average_pooling3d (G  (None, 32)               0         \n",
      " lobalAveragePooling3D)                                          \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 20)                2020      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 20)               80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 20)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 624,768\n",
      "Trainable params: 624,328\n",
      "Non-trainable params: 440\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hp_name = \"hp_conv1dse_test\"\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=500,\n",
    "#     hyperband_iterations=2,\n",
    "#     directory=f\"./my_hp_results/{hp_name}\",\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "# tuner.search(train,\n",
    "#     train_target,\n",
    "#     validation_data=(validation,validation_target),\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     verbose=2,\n",
    "#     callbacks=[EarlyStopping(patience=1)])\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory=f\"./my_hp_results/{hp_name}_bayesian\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357f2af9-11cf-4350-a804-738d58d56de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 01s]\n",
      "\n",
      "Best val_accuracy So Far: 0.3738125115633011\n",
      "Total elapsed time: 00h 32m 50s\n",
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |1                 |num_layers\n",
      "0.001983          |7.9538e-05        |kernel_regulizer_l2\n",
      "0.00019192        |0.010572          |recurrent_regulizer_l2\n",
      "1.9885e-06        |0.00014611        |dense_regulizer_l2\n",
      "0.2               |0.55              |dropout\n",
      "0.5               |0.8               |dropout_dense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n",
      "    model = self._try_build(hp)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"/tmp/ipykernel_8551/198724820.py\", line 54, in build_model\n",
      "    model.add(GlobalAveragePooling3D())\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/trackable/base.py\", line 205, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: Input 0 of layer \"global_average_pooling3d\" is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: (None, 1, 1500, 64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/tmp/ipykernel_8551/198724820.py\", line 54, in build_model\n    model.add(GlobalAveragePooling3D())\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/trackable/base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n    raise ValueError(\nValueError: Input 0 of layer \"global_average_pooling3d\" is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: (None, 1, 1500, 64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:231\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:335\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# Display needs the updated trial scored by the Oracle.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display\u001b[38;5;241m.\u001b[39mon_trial_end(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id))\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    106\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 107\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    109\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/oracle.py:434\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 434\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras_tuner/engine/oracle.py:386\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures excceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;241m+\u001b[39m trial\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/tmp/ipykernel_8551/198724820.py\", line 54, in build_model\n    model.add(GlobalAveragePooling3D())\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/tensorflow/python/trackable/base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/rezmiry/venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n    raise ValueError(\nValueError: Input 0 of layer \"global_average_pooling3d\" is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: (None, 1, 1500, 64)\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train,\n",
    "    train_target,\n",
    "    validation_data=(validation,validation_target),\n",
    "    epochs=30,\n",
    "    batch_size=512,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64873c-5f5f-4e5e-acce-48079635458b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
